{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "def split_dataset(dataset_path, part1_path, part2_path):\n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(part1_path, exist_ok=True)\n",
    "    os.makedirs(part2_path, exist_ok=True)\n",
    "\n",
    "    # List all images in the dataset\n",
    "    images = [f for f in os.listdir(dataset_path) if os.path.isfile(os.path.join(dataset_path, f))]\n",
    "    random.shuffle(images)\n",
    "\n",
    "    # Split the dataset into two equal parts\n",
    "    split_index = len(images) // 2\n",
    "    part1_images = images[:split_index]\n",
    "    part2_images = images[split_index:]\n",
    "\n",
    "    # Move images to their respective directories\n",
    "    for img in part1_images:\n",
    "        shutil.move(os.path.join(dataset_path, img), os.path.join(part1_path, img))\n",
    "    for img in part2_images:\n",
    "        shutil.move(os.path.join(dataset_path, img), os.path.join(part2_path, img))\n",
    "\n",
    "    print(f\"Dataset split into {part1_path} and {part2_path}\")\n",
    "\n",
    "# Example usage\n",
    "split_dataset('Datasets\\places2', 'Datasets\\split1', 'Datasets/split2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install matplotlib pandas numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip show torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m ipykernel install --user --name=your_env --display-name \"Python (venv)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import PIL\n",
    "import numpy\n",
    "import matplotlib.pyplot\n",
    "print(\"All imports work!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run on CUDA\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Define Generator Network (simplified)\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, channels=3):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        # Initial layer\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(channels + 1, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Encoder\n",
    "        self.down1 = self._down_block(64, 128)\n",
    "        self.down2 = self._down_block(128, 256)\n",
    "        self.down3 = self._down_block(256, 512)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        self.up1 = self._up_block(512, 512)\n",
    "        self.up2 = self._up_block(512 * 2, 256)\n",
    "        self.up3 = self._up_block(256 * 2, 128)\n",
    "        self.up4 = self._up_block(128 * 2, 64)\n",
    "        \n",
    "        # Final layer\n",
    "        self.final = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64 * 2, channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def _down_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "    \n",
    "    def _up_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        # Concatenate image with mask\n",
    "        x = torch.cat([x, mask], dim=1)\n",
    "        \n",
    "        # Encoder\n",
    "        d1 = self.initial(x)\n",
    "        d2 = self.down1(d1)\n",
    "        d3 = self.down2(d2)\n",
    "        d4 = self.down3(d3)\n",
    "        bottleneck = self.bottleneck(d4)\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        u1 = self.up1(bottleneck)\n",
    "        u2 = self.up2(torch.cat([u1, d4], dim=1))\n",
    "        u3 = self.up3(torch.cat([u2, d3], dim=1))\n",
    "        u4 = self.up4(torch.cat([u3, d2], dim=1))\n",
    "        \n",
    "        return self.final(torch.cat([u4, d1], dim=1))\n",
    "\n",
    "# Simplified Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        # Simplified PatchGAN discriminator\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(channels + 1, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        x = torch.cat([x, mask], dim=1)\n",
    "        return self.model(x)\n",
    "\n",
    "# Simplified mask creation function\n",
    "def create_mask(image_shape, min_percentage=0.10, max_percentage=0.30):\n",
    "    height, width = image_shape[:2]\n",
    "    total_pixels = height * width\n",
    "    \n",
    "    # Calculate masked area\n",
    "    target_pixels = int(total_pixels * random.uniform(min_percentage, max_percentage))\n",
    "    \n",
    "    # Create rectangular mask\n",
    "    mask = np.zeros((height, width), dtype=np.float32)\n",
    "    \n",
    "    # Random mask type\n",
    "    mask_type = random.choice(['rectangle', 'multiple'])\n",
    "    \n",
    "    if mask_type == 'rectangle':\n",
    "        # Single rectangular region\n",
    "        max_side = int(np.sqrt(target_pixels))\n",
    "        rect_width = random.randint(max_side // 2, max_side)\n",
    "        rect_height = target_pixels // rect_width\n",
    "        \n",
    "        x = random.randint(0, width - rect_width)\n",
    "        y = random.randint(0, height - rect_height)\n",
    "        \n",
    "        mask[y:y+rect_height, x:x+rect_width] = 1.0\n",
    "    else:\n",
    "        # Multiple masked regions\n",
    "        num_regions = random.randint(2, 4)\n",
    "        pixels_per_region = target_pixels // num_regions\n",
    "        \n",
    "        for _ in range(num_regions):\n",
    "            region_size = int(np.sqrt(pixels_per_region))\n",
    "            x = random.randint(0, width - region_size)\n",
    "            y = random.randint(0, height - region_size)\n",
    "            \n",
    "            mask[y:y+region_size, x:x+region_size] = 1.0\n",
    "            \n",
    "    return mask\n",
    "\n",
    "# Simplified Dataset (assumes pre-processed images)\n",
    "class InpaintingDataset(Dataset):\n",
    "    def __init__(self, image_dir, image_size=256, min_mask_percentage=0.10, max_mask_percentage=0.30, max_images=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.image_size = image_size\n",
    "        self.min_mask_percentage = min_mask_percentage\n",
    "        self.max_mask_percentage = max_mask_percentage\n",
    "        \n",
    "        # Get image paths\n",
    "        self.image_paths = []\n",
    "        for root, _, files in os.walk(image_dir):\n",
    "            for filename in files:\n",
    "                if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
    "                    self.image_paths.append(os.path.join(root, filename))\n",
    "        \n",
    "        # Limit dataset size if specified\n",
    "        if max_images:\n",
    "            self.image_paths = self.image_paths[:min(len(self.image_paths), max_images)]\n",
    "        \n",
    "        # Basic normalization only\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image (assuming already preprocessed)\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        image_tensor = self.transform(image)\n",
    "        \n",
    "        # Create mask\n",
    "        mask = create_mask(\n",
    "            (self.image_size, self.image_size),\n",
    "            self.min_mask_percentage,\n",
    "            self.max_mask_percentage\n",
    "        )\n",
    "        mask_tensor = torch.from_numpy(mask).unsqueeze(0).float()\n",
    "        \n",
    "        # Apply mask\n",
    "        corrupted = image_tensor * (1 - mask_tensor)\n",
    "        \n",
    "        return {\n",
    "            'original': image_tensor,\n",
    "            'corrupted': corrupted,\n",
    "            'mask': mask_tensor,\n",
    "            'path': self.image_paths[idx]\n",
    "        }\n",
    "\n",
    "# Simplified training function\n",
    "def train_gan(input_dir, output_dir, epochs=30, batch_size=8, lr=0.0002, \n",
    "              min_mask=0.10, max_mask=0.30, max_images=None):\n",
    "    \n",
    "    # Create output directories\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, 'models'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, 'samples'), exist_ok=True)\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Initialize models\n",
    "    generator = Generator().to(device)\n",
    "    discriminator = Discriminator().to(device)\n",
    "    \n",
    "    # Optimizers\n",
    "    g_optimizer = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    d_optimizer = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    \n",
    "    # Loss functions\n",
    "    adversarial_loss = nn.BCEWithLogitsLoss()\n",
    "    l1_loss = nn.L1Loss()\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = InpaintingDataset(\n",
    "        input_dir, \n",
    "        min_mask_percentage=min_mask,\n",
    "        max_mask_percentage=max_mask,\n",
    "        max_images=max_images\n",
    "    )\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    \n",
    "    print(f\"Training with {len(dataset)} images\")\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            # Get data\n",
    "            real_images = batch['original'].to(device)\n",
    "            corrupted_images = batch['corrupted'].to(device)\n",
    "            masks = batch['mask'].to(device)\n",
    "            \n",
    "            # Labels for adversarial loss\n",
    "            valid = torch.ones((real_images.size(0), 1, 16, 16), device=device)\n",
    "            fake = torch.zeros((real_images.size(0), 1, 16, 16), device=device)\n",
    "            \n",
    "            # Train Generator\n",
    "            g_optimizer.zero_grad()\n",
    "            \n",
    "            # Generate inpainted images\n",
    "            gen_images = generator(corrupted_images, masks)\n",
    "            combined = gen_images * masks + real_images * (1 - masks)\n",
    "            \n",
    "            # Discriminator evaluation\n",
    "            pred_fake = discriminator(combined, masks)\n",
    "            \n",
    "            # Generator losses\n",
    "            g_adv_loss = adversarial_loss(pred_fake, valid)\n",
    "            g_l1_loss = l1_loss(combined, real_images)\n",
    "            g_loss = g_adv_loss + 100 * g_l1_loss\n",
    "            \n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "            \n",
    "            # Train Discriminator (less frequently)\n",
    "            if batch_idx % 3 == 0:\n",
    "                d_optimizer.zero_grad()\n",
    "                \n",
    "                # Real loss\n",
    "                pred_real = discriminator(real_images, masks)\n",
    "                d_real_loss = adversarial_loss(pred_real, valid)\n",
    "                \n",
    "                # Fake loss\n",
    "                pred_fake = discriminator(combined.detach(), masks)\n",
    "                d_fake_loss = adversarial_loss(pred_fake, fake)\n",
    "                \n",
    "                d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "                d_loss.backward()\n",
    "                d_optimizer.step()\n",
    "            else:\n",
    "                d_loss = torch.tensor(0.0)\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix(\n",
    "                g_loss=g_loss.item(), \n",
    "                d_loss=d_loss.item(), \n",
    "                l1_loss=g_l1_loss.item()\n",
    "            )\n",
    "        \n",
    "        # Save model checkpoints and samples\n",
    "        if (epoch + 1) % 5 == 0 or epoch == epochs - 1:\n",
    "            # Save model\n",
    "            torch.save({\n",
    "                'generator': generator.state_dict(),\n",
    "                'discriminator': discriminator.state_dict(),\n",
    "                'epoch': epoch\n",
    "            }, os.path.join(output_dir, 'models', f'inpainting_epoch_{epoch+1}.pth'))\n",
    "            \n",
    "            # Save sample images\n",
    "            with torch.no_grad():\n",
    "                sample_idx = random.randint(0, len(dataset) - 1)\n",
    "                sample = dataset[sample_idx]\n",
    "                \n",
    "                real = sample['original'].unsqueeze(0).to(device)\n",
    "                corrupted = sample['corrupted'].unsqueeze(0).to(device)\n",
    "                mask = sample['mask'].unsqueeze(0).to(device)\n",
    "                \n",
    "                gen = generator(corrupted, mask)\n",
    "                combined = gen * mask + real * (1 - mask)\n",
    "                \n",
    "                # Save samples\n",
    "                samples = torch.cat([\n",
    "                    corrupted,  # Masked image\n",
    "                    combined,   # Inpainted result\n",
    "                    real        # Original\n",
    "                ], dim=0)\n",
    "                samples = (samples * 0.5 + 0.5).clamp(0, 1)\n",
    "                \n",
    "                save_image(\n",
    "                    samples,\n",
    "                    os.path.join(output_dir, 'samples', f'epoch_{epoch+1}.png'),\n",
    "                    nrow=3,\n",
    "                    normalize=False\n",
    "                )\n",
    "\n",
    "# Simplified inference function\n",
    "def inpaint_images(model_path, input_dir, output_dir, min_mask=0.10, max_mask=0.30, max_images=None):\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Load model\n",
    "    generator = Generator().to(device)\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    generator.load_state_dict(checkpoint['generator'])\n",
    "    generator.eval()\n",
    "    \n",
    "    # Transform\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "    # Get images\n",
    "    image_files = []\n",
    "    for root, _, files in os.walk(input_dir):\n",
    "        for filename in files:\n",
    "            if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
    "                image_files.append(os.path.join(root, filename))\n",
    "    \n",
    "    # Limit images\n",
    "    if max_images:\n",
    "        image_files = image_files[:min(len(image_files), max_images)]\n",
    "    \n",
    "    # Process images\n",
    "    for image_path in tqdm(image_files, desc=\"Inpainting\"):\n",
    "        try:\n",
    "            # Load image\n",
    "            original = Image.open(image_path).convert('RGB')\n",
    "            original_size = original.size\n",
    "            \n",
    "            # Transform\n",
    "            img_tensor = transform(original).unsqueeze(0).to(device)\n",
    "            \n",
    "            # Create mask\n",
    "            mask_np = create_mask((256, 256), min_mask, max_mask)\n",
    "            mask_tensor = torch.from_numpy(mask_np).unsqueeze(0).unsqueeze(0).to(device).float()\n",
    "            \n",
    "            # Corrupt image\n",
    "            corrupted = img_tensor * (1 - mask_tensor)\n",
    "            \n",
    "            # Generate inpainting\n",
    "            with torch.no_grad():\n",
    "                gen_img = generator(corrupted, mask_tensor)\n",
    "                result = gen_img * mask_tensor + img_tensor * (1 - mask_tensor)\n",
    "            \n",
    "            # Convert to image\n",
    "            result_img = (result.squeeze(0).cpu() * 0.5 + 0.5).clamp(0, 1)\n",
    "            result_img = transforms.ToPILImage()(result_img)\n",
    "            result_img = result_img.resize(original_size)\n",
    "            \n",
    "            # Save\n",
    "            filename = os.path.basename(image_path)\n",
    "            result_img.save(os.path.join(output_dir, f'inpainted_{filename}'))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_path}: {str(e)}\")\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Train\n",
    "    train_gan(\n",
    "        input_dir='Datasets/split2',  # Change to your input directory\n",
    "        output_dir='Models/gan_inpaint_model/models',\n",
    "        epochs=30,\n",
    "        batch_size=8,\n",
    "        min_mask=0.10,\n",
    "        max_mask=0.30,\n",
    "        max_images=2500\n",
    "    )\n",
    "    \n",
    "    # Inpaint\n",
    "    inpaint_images(\n",
    "        model_path='Models/gan_inpaint_model/models/inpainting_epoch_30.pth',\n",
    "        input_dir='Datasets/split2',  # Change to your test directory\n",
    "        output_dir='Datasets/gan_in_split2',\n",
    "        min_mask=0.10,\n",
    "        max_mask=0.30,\n",
    "        max_images=2500\n",
    "    )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "def label_and_shuffle(part1_dir, part2_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    labeled_images = []\n",
    "    for img_name in os.listdir(part1_dir):\n",
    "        labeled_images.append((os.path.join(part1_dir, img_name), 0))\n",
    "    for img_name in os.listdir(part2_dir):\n",
    "        labeled_images.append((os.path.join(part2_dir, img_name), 1))\n",
    "    \n",
    "    random.shuffle(labeled_images)\n",
    "    \n",
    "    for i, (img_path, label) in enumerate(labeled_images):\n",
    "        img_name = f\"{i}_{label}.png\"\n",
    "        shutil.copy(img_path, os.path.join(output_dir, img_name))\n",
    "    \n",
    "    print(\"Dataset labeled and shuffled\")\n",
    "\n",
    "# Example usage\n",
    "label_and_shuffle('path/to/part1', 'path/to/part2_inpainted', 'path/to/shuffled_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "def split_train_test(shuffled_dataset_dir, train_dir, test_dir):\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "    \n",
    "    images = [f for f in os.listdir(shuffled_dataset_dir) if os.path.isfile(os.path.join(shuffled_dataset_dir, f))]\n",
    "    random.shuffle(images)\n",
    "    \n",
    "    split_index = len(images) // 2\n",
    "    train_images = images[:split_index]\n",
    "    test_images = images[split_index:]\n",
    "    \n",
    "    for img in train_images:\n",
    "        shutil.move(os.path.join(shuffled_dataset_dir, img), os.path.join(train_dir, img))\n",
    "    for img in test_images:\n",
    "        shutil.move(os.path.join(shuffled_dataset_dir, img), os.path.join(test_dir, img))\n",
    "    \n",
    "    print(f\"Dataset split into training and testing sets\")\n",
    "\n",
    "# Example usage\n",
    "split_train_test('path/to/shuffled_dataset', 'path/to/train_set', 'path/to/test_set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define the model\n",
    "def create_model():\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(2, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Data generators for training and testing sets\n",
    "train_datagen = ImageDataGenerator(rescale=0.5)\n",
    "test_datagen = ImageDataGenerator(rescale=0.5)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'path/to/train_set',\n",
    "    target_size=(128, 128),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    'path/to/test_set',\n",
    "    target_size=(128, 128),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "# Create and train the model\n",
    "model = create_model()\n",
    "model.fit(train_generator, epochs=10, validation_data=test_generator)\n",
    "\n",
    "# Save the model as an H5 file\n",
    "model.save('path/to/save/model.h5')\n",
    "print(\"Model saved as model.h5\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
