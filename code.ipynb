{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split into Github/inpainting_and_forgery_detection/split1 and Github/inpainting_and_forgery_detection/split2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "def split_dataset(dataset_path, part1_path, part2_path):\n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(part1_path, exist_ok=True)\n",
    "    os.makedirs(part2_path, exist_ok=True)\n",
    "\n",
    "    # List all images in the dataset\n",
    "    images = [f for f in os.listdir(dataset_path) if os.path.isfile(os.path.join(dataset_path, f))]\n",
    "    random.shuffle(images)\n",
    "\n",
    "    # Split the dataset into two equal parts\n",
    "    split_index = len(images) // 2\n",
    "    part1_images = images[:split_index]\n",
    "    part2_images = images[split_index:]\n",
    "\n",
    "    # Move images to their respective directories\n",
    "    for img in part1_images:\n",
    "        shutil.move(os.path.join(dataset_path, img), os.path.join(part1_path, img))\n",
    "    for img in part2_images:\n",
    "        shutil.move(os.path.join(dataset_path, img), os.path.join(part2_path, img))\n",
    "\n",
    "    print(f\"Dataset split into {part1_path} and {part2_path}\")\n",
    "\n",
    "# Example usage\n",
    "split_dataset('Datasets\\places2', 'Datasets\\split1', 'Datasets/split2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.6.0-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.21.0-cp312-cp312-win_amd64.whl.metadata (6.3 kB)\n",
      "Collecting torchaudio\n",
      "  Using cached torchaudio-2.6.0-cp312-cp312-win_amd64.whl.metadata (6.7 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting typing-extensions>=4.10.0 (from torch)\n",
      "  Downloading typing_extensions-4.13.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting setuptools (from torch)\n",
      "  Downloading setuptools-78.1.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting numpy (from torchvision)\n",
      "  Using cached numpy-2.2.4-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n",
      "  Using cached pillow-11.1.0-cp312-cp312-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Using cached MarkupSafe-3.0.2-cp312-cp312-win_amd64.whl.metadata (4.1 kB)\n",
      "Using cached torch-2.6.0-cp312-cp312-win_amd64.whl (204.1 MB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached torchvision-0.21.0-cp312-cp312-win_amd64.whl (1.6 MB)\n",
      "Using cached torchaudio-2.6.0-cp312-cp312-win_amd64.whl (2.4 MB)\n",
      "Using cached pillow-11.1.0-cp312-cp312-win_amd64.whl (2.6 MB)\n",
      "Downloading typing_extensions-4.13.0-py3-none-any.whl (45 kB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached numpy-2.2.4-cp312-cp312-win_amd64.whl (12.6 MB)\n",
      "Downloading setuptools-78.1.0-py3-none-any.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 0.5/1.3 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.3/1.3 MB 4.5 MB/s eta 0:00:00\n",
      "Using cached MarkupSafe-3.0.2-cp312-cp312-win_amd64.whl (15 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, typing-extensions, sympy, setuptools, pillow, numpy, networkx, MarkupSafe, fsspec, filelock, jinja2, torch, torchvision, torchaudio\n",
      "Successfully installed MarkupSafe-3.0.2 filelock-3.18.0 fsspec-2025.3.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.4.2 numpy-2.2.4 pillow-11.1.0 setuptools-78.1.0 sympy-1.13.1 torch-2.6.0 torchaudio-2.6.0 torchvision-0.21.0 typing-extensions-4.13.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.10.1-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\revna\\desktop\\github\\inpainting_and_forgery_detection\\venv\\lib\\site-packages (2.2.4)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.3.1-cp312-cp312-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Using cached fonttools-4.56.0-cp312-cp312-win_amd64.whl.metadata (103 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.8-cp312-cp312-win_amd64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\revna\\desktop\\github\\inpainting_and_forgery_detection\\venv\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\revna\\desktop\\github\\inpainting_and_forgery_detection\\venv\\lib\\site-packages (from matplotlib) (11.1.0)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\revna\\desktop\\github\\inpainting_and_forgery_detection\\venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\revna\\desktop\\github\\inpainting_and_forgery_detection\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Using cached matplotlib-3.10.1-cp312-cp312-win_amd64.whl (8.1 MB)\n",
      "Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "Using cached contourpy-1.3.1-cp312-cp312-win_amd64.whl (220 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.56.0-cp312-cp312-win_amd64.whl (2.2 MB)\n",
      "Using cached kiwisolver-1.4.8-cp312-cp312-win_amd64.whl (71 kB)\n",
      "Downloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, pyparsing, kiwisolver, fonttools, cycler, contourpy, pandas, matplotlib\n",
      "Successfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.56.0 kiwisolver-1.4.8 matplotlib-3.10.1 pandas-2.2.3 pyparsing-3.2.3 pytz-2025.2 tzdata-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib pandas numpy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.11.0.86-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\revna\\desktop\\github\\inpainting_and_forgery_detection\\venv\\lib\\site-packages (from opencv-python) (2.2.4)\n",
      "Downloading opencv_python-4.11.0.86-cp37-abi3-win_amd64.whl (39.5 MB)\n",
      "   ---------------------------------------- 0.0/39.5 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 2.9/39.5 MB 14.0 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 6.6/39.5 MB 15.5 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 9.4/39.5 MB 15.1 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 12.3/39.5 MB 14.3 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 15.2/39.5 MB 14.3 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 17.8/39.5 MB 13.9 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 20.7/39.5 MB 13.8 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 23.3/39.5 MB 13.7 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 26.2/39.5 MB 13.6 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 28.8/39.5 MB 13.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 31.5/39.5 MB 13.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 34.3/39.5 MB 13.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 37.2/39.5 MB 13.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  39.3/39.5 MB 13.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 39.5/39.5 MB 12.7 MB/s eta 0:00:00\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.11.0.86\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install opencv-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torch\n",
      "Version: 2.6.0\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3-Clause\n",
      "Location: c:\\Users\\revna\\Desktop\\Github\\Inpainting_and_Forgery_Detection\\venv\\Lib\\site-packages\n",
      "Requires: filelock, fsspec, jinja2, networkx, setuptools, sympy, typing-extensions\n",
      "Required-by: torchaudio, torchvision\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip show torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed kernelspec your_env in C:\\Users\\revna\\AppData\\Roaming\\jupyter\\kernels\\your_env\n"
     ]
    }
   ],
   "source": [
    "!python -m ipykernel install --user --name=your_env --display-name \"Python (venv)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports work!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import PIL\n",
    "import numpy\n",
    "import matplotlib.pyplot\n",
    "print(\"All imports work!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\revna\\desktop\\github\\inpainting_and_forgery_detection\\venv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 5000 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inpainting Images: 100%|██████████| 5000/5000 [20:46<00:00,  4.01it/s]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm  # For progress tracking\n",
    "\n",
    "def create_mask(image_shape, min_mask_percentage=0.10, max_mask_percentage=0.30):\n",
    "    \"\"\"\n",
    "    Create a more dynamic mask with controlled total masked area between min and max percentages\n",
    "    \n",
    "    Args:\n",
    "    - image_shape: Shape of the image (height, width)\n",
    "    - min_mask_percentage: Minimum percentage of image to be masked (default 10%)\n",
    "    - max_mask_percentage: Maximum percentage of image to be masked (default 30%)\n",
    "    \n",
    "    Returns:\n",
    "    - mask: Numpy array with mask (255 for masked regions)\n",
    "    \"\"\"\n",
    "    height, width = image_shape[:2]\n",
    "    total_pixels = height * width\n",
    "    \n",
    "    # Ensure min is not greater than max\n",
    "    min_mask_percentage = min(min_mask_percentage, max_mask_percentage)\n",
    "    \n",
    "    # Calculate target masked pixels within the range\n",
    "    min_masked_pixels = int(total_pixels * min_mask_percentage)\n",
    "    max_masked_pixels = int(total_pixels * max_mask_percentage)\n",
    "    target_masked_pixels = np.random.randint(min_masked_pixels, max_masked_pixels + 1)\n",
    "    \n",
    "    # Initialize mask\n",
    "    mask = np.zeros((height, width), dtype=np.uint8)\n",
    "    \n",
    "    # Randomize mask generation approach\n",
    "    mask_type = np.random.choice(['rectangle', 'multiple', 'irregular'])\n",
    "    \n",
    "    if mask_type == 'rectangle':\n",
    "        # Single rectangular region\n",
    "        max_side = int(np.sqrt(target_masked_pixels))\n",
    "        rect_width = np.random.randint(max_side // 2, max_side)\n",
    "        rect_height = target_masked_pixels // rect_width\n",
    "        \n",
    "        x = np.random.randint(0, width - rect_width)\n",
    "        y = np.random.randint(0, height - rect_height)\n",
    "        \n",
    "        mask[y:y+rect_height, x:x+rect_width] = 255\n",
    "    \n",
    "    elif mask_type == 'multiple':\n",
    "        # Multiple smaller masked regions\n",
    "        num_regions = np.random.randint(2, 6)\n",
    "        pixels_per_region = target_masked_pixels // num_regions\n",
    "        \n",
    "        for _ in range(num_regions):\n",
    "            region_size = int(np.sqrt(pixels_per_region))\n",
    "            x = np.random.randint(0, width - region_size)\n",
    "            y = np.random.randint(0, height - region_size)\n",
    "            \n",
    "            mask[y:y+region_size, x:x+region_size] = 255\n",
    "    \n",
    "    else:  # irregular mask\n",
    "        # Create an irregular mask using random walk\n",
    "        current_masked_pixels = 0\n",
    "        max_attempts = 1000\n",
    "        attempts = 0\n",
    "        \n",
    "        while current_masked_pixels < target_masked_pixels and attempts < max_attempts:\n",
    "            # Random walk\n",
    "            x, y = np.random.randint(0, width), np.random.randint(0, height)\n",
    "            step_size = np.random.randint(1, 10)\n",
    "            \n",
    "            # Ensure we don't go out of bounds\n",
    "            x = max(0, min(x, width-1))\n",
    "            y = max(0, min(y, height-1))\n",
    "            \n",
    "            # Create a small irregular region\n",
    "            region = mask[max(0, y-step_size):min(height, y+step_size),\n",
    "                          max(0, x-step_size):min(width, x+step_size)]\n",
    "            \n",
    "            # Add region to mask if not already masked\n",
    "            new_mask_pixels = np.sum(region == 0)\n",
    "            if new_mask_pixels > 0:\n",
    "                region[region == 0] = 255\n",
    "                current_masked_pixels += new_mask_pixels\n",
    "            \n",
    "            attempts += 1\n",
    "    \n",
    "    # Verify mask percentage (optional - can be commented out for bulk processing)\n",
    "    # masked_percentage = np.sum(mask == 255) / total_pixels\n",
    "    # print(f\"Mask Type: {mask_type}, Masked Percentage: {masked_percentage:.2%}, \" \n",
    "    #       f\"Range: {min_mask_percentage:.2%}-{max_mask_percentage:.2%}\")\n",
    "    \n",
    "    return mask\n",
    "\n",
    "\n",
    "def inpaint_image(image_path, output_dir, min_mask_percentage=0.05, max_mask_percentage=0.15, save_visualization=False):\n",
    "    \"\"\"\n",
    "    Inpaint an image using OpenCV's Telea inpainting method\n",
    "    \n",
    "    Args:\n",
    "    - image_path: Path to input image\n",
    "    - output_dir: Directory to save output images\n",
    "    - min_mask_percentage: Minimum percentage of image to be masked\n",
    "    - max_mask_percentage: Maximum percentage of image to be masked\n",
    "    - save_visualization: Whether to save the visualization (False for bulk processing)\n",
    "    \"\"\"\n",
    "    # Create output directories\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    if save_visualization:\n",
    "        os.makedirs(os.path.join(output_dir, 'visualizations'), exist_ok=True)\n",
    "    \n",
    "    # Read the image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Warning: Couldn't read image {image_path}\")\n",
    "        return\n",
    "    \n",
    "    # Create a mask with specified min/max percentage\n",
    "    mask = create_mask(image.shape, min_mask_percentage, max_mask_percentage)\n",
    "    \n",
    "    # Telea Method\n",
    "    inpainted_telea = cv2.inpaint(image, mask, inpaintRadius=3, flags=cv2.INPAINT_TELEA)\n",
    "    \n",
    "    # Save inpainted image\n",
    "    filename = os.path.basename(image_path)\n",
    "    cv2.imwrite(os.path.join(output_dir, f'telea_{filename}'), inpainted_telea)\n",
    "    \n",
    "    # Save visualization only if requested (disabled for bulk processing)\n",
    "    if save_visualization:\n",
    "        # Create visualization figure\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        # Original Image\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.title('Original Image')\n",
    "        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Mask Visualization\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.title('Mask')\n",
    "        plt.imshow(mask, cmap='gray')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Telea Inpainting\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.title('Telea Inpainting')\n",
    "        plt.imshow(cv2.cvtColor(inpainted_telea, cv2.COLOR_BGR2RGB))\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Save visualization\n",
    "        vis_path = os.path.join(output_dir, 'visualizations', f'inpainting_{filename}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(vis_path)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def process_directory(input_dir, output_dir, max_images=10000, min_mask_percentage=0.05, max_mask_percentage=0.15):\n",
    "    \"\"\"\n",
    "    Process up to max_images images in a directory\n",
    "    \n",
    "    Args:\n",
    "    - input_dir: Directory with input images\n",
    "    - output_dir: Directory to save output images\n",
    "    - max_images: Maximum number of images to process\n",
    "    - min_mask_percentage: Minimum percentage of image to be masked\n",
    "    - max_mask_percentage: Maximum percentage of image to be masked\n",
    "    \"\"\"\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Get list of all image files\n",
    "    image_files = []\n",
    "    for root, _, files in os.walk(input_dir):\n",
    "        for filename in files:\n",
    "            if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
    "                image_files.append(os.path.join(root, filename))\n",
    "    \n",
    "    # Limit to max_images\n",
    "    total_images = min(len(image_files), max_images)\n",
    "    if len(image_files) < max_images:\n",
    "        print(f\"Warning: Only found {len(image_files)} images, fewer than the requested {max_images}\")\n",
    "    else:\n",
    "        print(f\"Processing {total_images} images...\")\n",
    "    \n",
    "    # Process images with progress bar\n",
    "    for image_path in tqdm(image_files[:total_images], desc=\"Inpainting Images\"):\n",
    "        try:\n",
    "            # Disable visualization for bulk processing to speed up\n",
    "            inpaint_image(\n",
    "                image_path=image_path,\n",
    "                output_dir=output_dir,\n",
    "                min_mask_percentage=min_mask_percentage,\n",
    "                max_mask_percentage=max_mask_percentage,\n",
    "                save_visualization=True  \n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_path}: {str(e)}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Process images\n",
    "    process_directory(\n",
    "        input_dir='Datasets/split2',  # Input image directory\n",
    "        output_dir='Datasets/in_split2',  # Output directory\n",
    "        max_images=5000,  # Process up to 5,000 images\n",
    "        min_mask_percentage=0.05,  # Minimum 5% of image will be masked\n",
    "        max_mask_percentage=0.15   # Maximum 15% of image will be masked\n",
    "    )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset labeled and shuffled - Used 5000 images from part1 and 5000 from part2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "def label_and_shuffle(part1_dir, part2_dir, output_dir, part1_limit):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Get images from part1 with limit\n",
    "    part1_images = os.listdir(part1_dir)\n",
    "    if len(part1_images) > part1_limit:\n",
    "        part1_images = part1_images[:part1_limit]\n",
    "    \n",
    "    labeled_images = []\n",
    "    for img_name in part1_images:\n",
    "        labeled_images.append((os.path.join(part1_dir, img_name), 0))\n",
    "    \n",
    "    # Get all images from part2\n",
    "    for img_name in os.listdir(part2_dir):\n",
    "        labeled_images.append((os.path.join(part2_dir, img_name), 1))\n",
    "    \n",
    "    random.shuffle(labeled_images)\n",
    "    \n",
    "    for i, (img_path, label) in enumerate(labeled_images):\n",
    "        img_name = f\"{i}_{label}.png\"\n",
    "        shutil.copy(img_path, os.path.join(output_dir, img_name))\n",
    "    \n",
    "    print(f\"Dataset labeled and shuffled - Used {len(part1_images)} images from part1 and {len(os.listdir(part2_dir))} from part2\")\n",
    "\n",
    "# Example usage\n",
    "label_and_shuffle('Datasets/split1', 'Datasets/in_split2', 'Datasets/shuffle_inpai_data', 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split into training and testing sets\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "def split_train_test(shuffled_dataset_dir, train_dir, test_dir):\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "    \n",
    "    images = [f for f in os.listdir(shuffled_dataset_dir) if os.path.isfile(os.path.join(shuffled_dataset_dir, f))]\n",
    "    random.shuffle(images)\n",
    "    \n",
    "    split_index = len(images) // 2\n",
    "    train_images = images[:split_index]\n",
    "    test_images = images[split_index:]\n",
    "    \n",
    "    for img in train_images:\n",
    "        shutil.move(os.path.join(shuffled_dataset_dir, img), os.path.join(train_dir, img))\n",
    "    for img in test_images:\n",
    "        shutil.move(os.path.join(shuffled_dataset_dir, img), os.path.join(test_dir, img))\n",
    "    \n",
    "    print(f\"Dataset split into training and testing sets\")\n",
    "\n",
    "# Example usage\n",
    "split_train_test('Datasets/shuffle_inpai_data', 'Datasets/norm_train', 'Datasets/norm_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading keras-3.9.1-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting absl-py (from keras)\n",
      "  Downloading absl_py-2.2.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\revna\\desktop\\github\\inpainting_and_forgery_detection\\venv\\lib\\site-packages (from keras) (2.2.4)\n",
      "Collecting rich (from keras)\n",
      "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras)\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting h5py (from keras)\n",
      "  Downloading h5py-3.13.0-cp312-cp312-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting optree (from keras)\n",
      "  Downloading optree-0.14.1-cp312-cp312-win_amd64.whl.metadata (50 kB)\n",
      "Collecting ml-dtypes (from keras)\n",
      "  Downloading ml_dtypes-0.5.1-cp312-cp312-win_amd64.whl.metadata (22 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\revna\\desktop\\github\\inpainting_and_forgery_detection\\venv\\lib\\site-packages (from keras) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\revna\\desktop\\github\\inpainting_and_forgery_detection\\venv\\lib\\site-packages (from optree->keras) (4.13.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\revna\\desktop\\github\\inpainting_and_forgery_detection\\venv\\lib\\site-packages (from rich->keras) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading keras-3.9.1-py3-none-any.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.3/1.3 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.3/1.3 MB ? eta -:--:--\n",
      "   ------------------------------- -------- 1.0/1.3 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.3/1.3 MB 1.6 MB/s eta 0:00:00\n",
      "Downloading absl_py-2.2.1-py3-none-any.whl (277 kB)\n",
      "Downloading h5py-3.13.0-cp312-cp312-win_amd64.whl (3.0 MB)\n",
      "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/3.0 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.5/3.0 MB 837.5 kB/s eta 0:00:03\n",
      "   -------------- ------------------------- 1.0/3.0 MB 1.3 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 1.6/3.0 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 2.1/3.0 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 2.4/3.0 MB 2.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.6/3.0 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.9/3.0 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.0/3.0 MB 1.5 MB/s eta 0:00:00\n",
      "Downloading ml_dtypes-0.5.1-cp312-cp312-win_amd64.whl (210 kB)\n",
      "Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.14.1-cp312-cp312-win_amd64.whl (306 kB)\n",
      "Downloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, optree, ml-dtypes, mdurl, h5py, absl-py, markdown-it-py, rich, keras\n",
      "Successfully installed absl-py-2.2.1 h5py-3.13.0 keras-3.9.1 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.5.1 namex-0.0.8 optree-0.14.1 rich-14.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.6.1-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\revna\\desktop\\github\\inpainting_and_forgery_detection\\venv\\lib\\site-packages (from scikit-learn) (2.2.4)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Using cached scipy-1.15.2-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached scikit_learn-1.6.1-cp312-cp312-win_amd64.whl (11.1 MB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached scipy-1.15.2-cp312-cp312-win_amd64.whl (40.9 MB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.6.1 scipy-1.15.2 threadpoolctl-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py in c:\\users\\revna\\desktop\\github\\inpainting_and_forgery_detection\\venv\\lib\\site-packages (3.13.0)\n",
      "Requirement already satisfied: numpy>=1.19.3 in c:\\users\\revna\\desktop\\github\\inpainting_and_forgery_detection\\venv\\lib\\site-packages (from h5py) (2.2.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Found 5000 images in Datasets/norm_train\n",
      "Found 5000 images in Datasets/norm_test\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import random\n",
    "import glob\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Custom Dataset class\n",
    "class ImageForgeryDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_dir (string): Directory with all the images and their corresponding labels\n",
    "            transform (callable, optional): Optional transform to be applied on a sample\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Get all image files\n",
    "        self.image_paths = []\n",
    "        for ext in ['*.jpg', '*.jpeg', '*.png']:\n",
    "            self.image_paths.extend(glob.glob(os.path.join(image_dir, ext)))\n",
    "        \n",
    "        print(f\"Found {len(self.image_paths)} images in {image_dir}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        # Extract label from filename - assuming filename ends with _0.jpg or _1.jpg\n",
    "        # where 0 = authentic, 1 = forged\n",
    "        filename = os.path.basename(image_path)\n",
    "        label = int(filename.split('_')[-1].split('.')[0])\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Define the Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim=100, channels=3):\n",
    "        super(Generator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            # Initial block\n",
    "            nn.ConvTranspose2d(latent_dim, 512, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # Second block\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # Third block\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # Fourth block\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # Output block\n",
    "            nn.ConvTranspose2d(64, channels, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = z.view(z.size(0), self.latent_dim, 1, 1)\n",
    "        return self.model(z)\n",
    "\n",
    "# Define the Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            # First block\n",
    "            nn.Conv2d(channels, 64, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Second block\n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Third block\n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Fourth block\n",
    "            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        \n",
    "        # Output layer for GAN discrimination (real/fake)\n",
    "        self.gan_output = nn.Sequential(\n",
    "            nn.Conv2d(512, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Output layer for classification (authentic/forged)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512 * 16 * 16, 512),  # Adjusted based on feature map size\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, img):\n",
    "        features = self.feature_extractor(img)\n",
    "        validity = self.gan_output(features)\n",
    "        classification = self.classifier(features)\n",
    "        return validity.view(-1), classification.view(-1)\n",
    "\n",
    "# Training function\n",
    "def train_gan_classifier(generator, discriminator, dataloader, epochs=50, latent_dim=100, \n",
    "                         lr_g=0.0002, lr_d=0.0002, betas=(0.5, 0.999), save_path='model'):\n",
    "    \n",
    "    # Loss functions\n",
    "    adversarial_loss = nn.BCELoss()\n",
    "    classification_loss = nn.BCELoss()\n",
    "    \n",
    "    # Optimizers\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=lr_g, betas=betas)\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=lr_d, betas=betas)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        for i, (real_imgs, labels) in enumerate(dataloader):\n",
    "            \n",
    "            batch_size = real_imgs.size(0)\n",
    "            real_imgs = real_imgs.to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            # Ground truths\n",
    "            valid = torch.ones(batch_size, 1, requires_grad=False).to(device)\n",
    "            fake = torch.zeros(batch_size, 1, requires_grad=False).to(device)\n",
    "            \n",
    "            # -----------------\n",
    "            #  Train Generator\n",
    "            # -----------------\n",
    "            optimizer_G.zero_grad()\n",
    "            \n",
    "            # Sample noise as generator input\n",
    "            z = torch.randn(batch_size, latent_dim).to(device)\n",
    "            \n",
    "            # Generate fake images\n",
    "            gen_imgs = generator(z)\n",
    "            \n",
    "            # Calculate loss and backpropagate\n",
    "            validity, _ = discriminator(gen_imgs)\n",
    "            g_loss = adversarial_loss(validity, valid)\n",
    "            \n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "            \n",
    "            # -------------------\n",
    "            #  Train Discriminator\n",
    "            # -------------------\n",
    "            optimizer_D.zero_grad()\n",
    "            \n",
    "            # Compute loss for real images\n",
    "            real_validity, real_pred = discriminator(real_imgs)\n",
    "            d_real_gan_loss = adversarial_loss(real_validity, valid)\n",
    "            d_real_class_loss = classification_loss(real_pred, labels)\n",
    "            \n",
    "            # Compute loss for fake images\n",
    "            fake_validity, _ = discriminator(gen_imgs.detach())\n",
    "            d_fake_gan_loss = adversarial_loss(fake_validity, fake)\n",
    "            \n",
    "            # Total discriminator loss\n",
    "            d_loss = d_real_gan_loss + d_fake_gan_loss + d_real_class_loss\n",
    "            \n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "            \n",
    "            # Print progress\n",
    "            if i % 10 == 0:\n",
    "                print(f\"[Epoch {epoch}/{epochs}] [Batch {i}/{len(dataloader)}] \"\n",
    "                      f\"[D loss: {d_loss.item():.4f}] [G loss: {g_loss.item():.4f}] \"\n",
    "                      f\"[Class loss: {d_real_class_loss.item():.4f}]\")\n",
    "        \n",
    "        # Save models periodically\n",
    "        if (epoch + 1) % 10 == 0 or epoch == epochs - 1:\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            torch.save(generator.state_dict(), f\"{save_path}/generator_epoch_{epoch+1}.pth\")\n",
    "            torch.save(discriminator.state_dict(), f\"{save_path}/discriminator_epoch_{epoch+1}.pth\")\n",
    "            print(f\"Models saved at epoch {epoch+1}\")\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(discriminator, dataloader):\n",
    "    discriminator.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in dataloader:\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            _, predictions = discriminator(imgs)\n",
    "            predictions = (predictions > 0.5).float()\n",
    "            \n",
    "            all_preds.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    \n",
    "    # Print detailed classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=['Authentic', 'Forged']))\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    tick_marks = [0, 1]\n",
    "    plt.xticks(tick_marks, ['Authentic', 'Forged'])\n",
    "    plt.yticks(tick_marks, ['Authentic', 'Forged'])\n",
    "    \n",
    "    # Add text annotations\n",
    "    thresh = cm.max() / 2\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# Main execution function\n",
    "def main():\n",
    "    # Data directories - update these paths to your data locations\n",
    "    train_dir = \"Datasets/norm_train\"\n",
    "    test_dir = \"Datasets/norm_test\"\n",
    "    model_save_path = \"Models/norm_det_model\"\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = ImageForgeryDataset(train_dir, transform=transform)\n",
    "    test_dataset = ImageForgeryDataset(test_dir, transform=transform)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "    \n",
    "    # Initialize models\n",
    "    latent_dim = 100\n",
    "    generator = Generator(latent_dim=latent_dim).to(device)\n",
    "    discriminator = Discriminator().to(device)\n",
    "    \n",
    "    # Train the models\n",
    "    train_gan_classifier(\n",
    "        generator=generator,\n",
    "        discriminator=discriminator,\n",
    "        dataloader=train_dataloader,\n",
    "        epochs=50,\n",
    "        latent_dim=latent_dim,\n",
    "        save_path=model_save_path\n",
    "    )\n",
    "    \n",
    "    # Load the best model for evaluation (typically the last one)\n",
    "    last_epoch = 50  # Change if using different number of epochs\n",
    "    discriminator.load_state_dict(torch.load(f\"{model_save_path}/discriminator_epoch_{last_epoch}.pth\"))\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = evaluate_model(discriminator, test_dataloader)\n",
    "    \n",
    "    # Save final model in h5 format (for compatibility)\n",
    "    try:\n",
    "        import h5py\n",
    "        torch.save(discriminator.state_dict(), f\"{model_save_path}/final_discriminator_model.h5\")\n",
    "        print(f\"Model saved in H5 format at {model_save_path}/final_discriminator_model.h5\")\n",
    "    except ImportError:\n",
    "        print(\"h5py not installed. Model saved in .pth format only.\")\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Reshape, Flatten, Conv2D, Conv2DTranspose, LeakyReLU, Dropout, Input\n",
    "from keras.optimizers import Adam\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset (replace with your own dataset)\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize and reshape data\n",
    "x_train = (x_train.astype(np.float32) - 127.5) / 127.5\n",
    "x_train = np.expand_dims(x_train, axis=3)\n",
    "x_test = (x_test.astype(np.float32) - 127.5) / 127.5\n",
    "x_test = np.expand_dims(x_test, axis=3)\n",
    "\n",
    "# Split a part of the training data for validation\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, stratify=y_train)\n",
    "\n",
    "# GAN Parameters\n",
    "latent_dim = 100\n",
    "img_shape = x_train.shape[1:]\n",
    "\n",
    "# Generator\n",
    "generator = Sequential([\n",
    "    Dense(128 * 7 * 7, activation=\"relu\", input_dim=latent_dim),\n",
    "    Reshape((7, 7, 128)),\n",
    "    Conv2DTranspose(128, kernel_size=4, strides=2, padding='same'),\n",
    "    LeakyReLU(alpha=0.2),\n",
    "    Conv2DTranspose(128, kernel_size=4, strides=2, padding='same'),\n",
    "    LeakyReLU(alpha=0.2),\n",
    "    Conv2D(1, kernel_size=7, activation='tanh', padding='same')\n",
    "])\n",
    "\n",
    "# Discriminator\n",
    "discriminator = Sequential([\n",
    "    Conv2D(64, kernel_size=3, strides=2, input_shape=img_shape, padding=\"same\"),\n",
    "    LeakyReLU(alpha=0.2),\n",
    "    Dropout(0.3),\n",
    "    Conv2D(128, kernel_size=3, strides=2, padding=\"same\"),\n",
    "    LeakyReLU(alpha=0.2),\n",
    "    Dropout(0.3),\n",
    "    Flatten(),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile Discriminator\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
    "\n",
    "# Combined model\n",
    "z = Input(shape=(latent_dim,))\n",
    "img = generator(z)\n",
    "discriminator.trainable = False\n",
    "validity = discriminator(img)\n",
    "combined = Model(z, validity)\n",
    "combined.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n",
    "\n",
    "# Training the GAN\n",
    "def train_gan(epochs, batch_size=128, save_interval=50):\n",
    "    valid = np.ones((batch_size, 1))\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Train Discriminator\n",
    "        idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
    "        imgs = x_train[idx]\n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        gen_imgs = generator.predict(noise)\n",
    "        d_loss_real = discriminator.train_on_batch(imgs, valid)\n",
    "        d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "        # Train Generator\n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        g_loss = combined.train_on_batch(noise, valid)\n",
    "\n",
    "        # Save the progress\n",
    "        if epoch % save_interval == 0:\n",
    "            print(f\"{epoch} [D loss: {d_loss[0]} | D accuracy: {100*d_loss[1]}] [G loss: {g_loss}]\")\n",
    "\n",
    "train_gan(epochs=10000, batch_size=64, save_interval=200)\n",
    "\n",
    "# Save the generator model\n",
    "generator.save('gan_generator.h5')\n",
    "\n",
    "# Generate augmented data\n",
    "noise = np.random.normal(0, 1, (10000, latent_dim))\n",
    "gen_imgs = generator.predict(noise)\n",
    "gen_imgs = (gen_imgs * 127.5 + 127.5).astype(np.uint8)\n",
    "\n",
    "# Append generated images to original training data\n",
    "x_train_aug = np.concatenate((x_train, gen_imgs))\n",
    "y_train_aug = np.concatenate((y_train, np.zeros(10000)))\n",
    "\n",
    "# Define and train the classifier\n",
    "classifier = Sequential([\n",
    "    Conv2D(32, kernel_size=3, activation='relu', input_shape=img_shape),\n",
    "    MaxPooling2D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "classifier.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "classifier.fit(x_train_aug, y_train_aug, epochs=10, batch_size=64, validation_data=(x_val, y_val))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = classifier.evaluate(x_test, y_test)\n",
    "print(f\"Test accuracy: {accuracy*100}%\")\n",
    "\n",
    "# Save the classifier model\n",
    "classifier.save('image_classifier.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "class ForgeryDetectionGAN:\n",
    "    def __init__(self, img_shape=(128, 128, 3), latent_dim=100):\n",
    "        self.img_shape = img_shape\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(\n",
    "            loss='binary_crossentropy',\n",
    "            optimizer=optimizers.Adam(learning_rate=0.0002, beta_1=0.5),\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "        \n",
    "        # The generator takes noise as input and generates images\n",
    "        z = layers.Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "        \n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "        \n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        validity = self.discriminator(img)\n",
    "        \n",
    "        # The combined model (stacks generator and discriminator)\n",
    "        self.combined = models.Model(z, validity)\n",
    "        self.combined.compile(\n",
    "            loss='binary_crossentropy',\n",
    "            optimizer=optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "        )\n",
    "        \n",
    "        # For classification\n",
    "        self.classifier = self.build_classifier()\n",
    "        self.classifier.compile(\n",
    "            loss='binary_crossentropy',\n",
    "            optimizer=optimizers.Adam(learning_rate=0.0002),\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "    \n",
    "    def build_generator(self):\n",
    "        model = models.Sequential()\n",
    "        \n",
    "        model.add(layers.Dense(128 * 32 * 32, activation=\"relu\", input_dim=self.latent_dim))\n",
    "        model.add(layers.Reshape((32, 32, 128)))\n",
    "        \n",
    "        model.add(layers.UpSampling2D())\n",
    "        model.add(layers.Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "        model.add(layers.BatchNormalization(momentum=0.8))\n",
    "        model.add(layers.Activation(\"relu\"))\n",
    "        \n",
    "        model.add(layers.UpSampling2D())\n",
    "        model.add(layers.Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "        model.add(layers.BatchNormalization(momentum=0.8))\n",
    "        model.add(layers.Activation(\"relu\"))\n",
    "        \n",
    "        model.add(layers.Conv2D(self.img_shape[2], kernel_size=3, padding=\"same\"))\n",
    "        model.add(layers.Activation(\"tanh\"))\n",
    "        \n",
    "        noise = layers.Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "        \n",
    "        return models.Model(noise, img)\n",
    "    \n",
    "    def build_discriminator(self):\n",
    "        model = models.Sequential()\n",
    "        \n",
    "        model.add(layers.Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
    "        model.add(layers.LeakyReLU(alpha=0.2))\n",
    "        model.add(layers.Dropout(0.25))\n",
    "        \n",
    "        model.add(layers.Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(layers.BatchNormalization(momentum=0.8))\n",
    "        model.add(layers.LeakyReLU(alpha=0.2))\n",
    "        model.add(layers.Dropout(0.25))\n",
    "        \n",
    "        model.add(layers.Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(layers.BatchNormalization(momentum=0.8))\n",
    "        model.add(layers.LeakyReLU(alpha=0.2))\n",
    "        model.add(layers.Dropout(0.25))\n",
    "        \n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(1, activation='sigmoid'))\n",
    "        \n",
    "        img = layers.Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "        \n",
    "        return models.Model(img, validity)\n",
    "    \n",
    "    def build_classifier(self):\n",
    "        # Using the discriminator as a feature extractor\n",
    "        feature_extractor = models.Sequential()\n",
    "        \n",
    "        # Copy layers from discriminator (excluding the final sigmoid layer)\n",
    "        for i in range(len(self.discriminator.layers) - 1):\n",
    "            feature_extractor.add(self.discriminator.layers[i])\n",
    "            \n",
    "        # Make these layers non-trainable\n",
    "        for layer in feature_extractor.layers:\n",
    "            layer.trainable = False\n",
    "            \n",
    "        # Add classification layers\n",
    "        model = models.Sequential()\n",
    "        model.add(feature_extractor)\n",
    "        model.add(layers.Dense(128, activation='relu'))\n",
    "        model.add(layers.Dropout(0.5))\n",
    "        model.add(layers.Dense(1, activation='sigmoid'))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def train_gan(self, X_train, epochs, batch_size=128):\n",
    "        # Labels for real and fake images\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Train Discriminator\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            real_imgs = X_train[idx]\n",
    "            \n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "            gen_imgs = self.generator.predict(noise, verbose=0)\n",
    "            \n",
    "            d_loss_real = self.discriminator.train_on_batch(real_imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "            \n",
    "            # Train Generator\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "            \n",
    "            if epoch % 200 == 0:\n",
    "                print(f\"Epoch {epoch}/{epochs} [D loss: {d_loss[0]:.4f}, acc.: {100*d_loss[1]:.2f}%] [G loss: {g_loss:.4f}]\")\n",
    "    \n",
    "    def train_classifier(self, X_train, y_train, X_val, y_val, epochs=20, batch_size=32):\n",
    "        # Train the classifier\n",
    "        history = self.classifier.fit(\n",
    "            X_train, y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_data=(X_val, y_val),\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def evaluate(self, X_test, y_test):\n",
    "        # Predict on test set\n",
    "        y_pred_prob = self.classifier.predict(X_test, verbose=0)\n",
    "        y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        # Print accuracy\n",
    "        print(f\"\\nACCURACY: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "        \n",
    "        return accuracy\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Parameters\n",
    "    img_size = (128, 128)\n",
    "    latent_dim = 100\n",
    "    batch_size = 32\n",
    "    gan_epochs = 1000\n",
    "    classifier_epochs = 20\n",
    "    \n",
    "    # Generate synthetic data for demonstration\n",
    "    # Replace this with your real data loading code\n",
    "    print(\"Generating synthetic data...\")\n",
    "    X = np.random.rand(1000, img_size[0], img_size[1], 3) * 2 - 1  # Normalized to [-1, 1]\n",
    "    y = np.random.randint(0, 2, size=(1000,))  # Binary labels (0: authentic, 1: forged)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "    \n",
    "    # Initialize and train the GAN\n",
    "    gan = ForgeryDetectionGAN(img_shape=(img_size[0], img_size[1], 3), latent_dim=latent_dim)\n",
    "    \n",
    "    print(\"Training GAN...\")\n",
    "    gan.train_gan(X_train, epochs=gan_epochs, batch_size=batch_size)\n",
    "    \n",
    "    print(\"\\nTraining classifier...\")\n",
    "    gan.train_classifier(X_train, y_train, X_val, y_val, epochs=classifier_epochs, batch_size=batch_size)\n",
    "    \n",
    "    print(\"\\nEvaluating model...\")\n",
    "    accuracy = gan.evaluate(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
